import streamlit as st
import pandas as pd
from datetime import datetime
from typing import List # List íƒ€ì… íŒíŠ¸ë¥¼ ìœ„í•´ ì¶”ê°€
import utils


def run_batch_analysis(app, chunks, similarity_threshold, vectorstore):
    """
    ì—¬ëŸ¬ ê°œì˜ ì¡°í•­(chunks)ì„ ìˆœíšŒí•˜ë©° ì¼ê´„ ë¶„ì„í•©ë‹ˆë‹¤.
    (HITLì´ ì—†ëŠ” ë‹¨ìˆœí•œ ì‹¤í–‰)
    """
    st.info(f"ì´ {len(chunks)}ê°œ ì¡°í•­ì— ëŒ€í•œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    progress_bar = st.progress(0, text="ë¶„ì„ ì§„í–‰ ì¤‘...")
    results = [] # ìµœì¢… ê²°ê³¼ ì €ì¥

    for i, chunk in enumerate(chunks):
        
        # ì±—ë´‡ ëª¨ë“œì™€ ë™ì¼í•˜ê²Œ thread_idë¥¼ ë§¤ë²ˆ ìƒˆë¡œ ìƒì„±
        thread_id = f"batch_session_{datetime.now().timestamp()}_{i}"
        config = {"configurable": {"thread_id": thread_id}}
        
        # ì±—ë´‡ ëª¨ë“œì™€ ë™ì¼í•˜ê²Œ initial_state êµ¬ì„±
        initial_state = {
            "clause": chunk,
            "iteration": 1,
            "session_id": thread_id,
            "validation_failed": False,
            "retrieved_cases_metadata": [],
            "retrieved_laws_metadata": [],
            "similarity_threshold": similarity_threshold
        }
        
        try:
            # 1. LangGraph appì„ ì§ì ‘ í˜¸ì¶œ (ë…¸ë“œ ìˆ˜ë™ í˜¸ì¶œì´ ì•„ë‹˜)
            # app.invoke()ëŠ” 'ê³µì •'/'ë¶ˆê³µì •'ì„ ì•Œì•„ì„œ íŒë‹¨í•˜ê³  ìµœì¢… ê²°ê³¼(output)ë¥¼ ë°˜í™˜
            output = app.invoke(initial_state, config=config)
            
            # 2. ê·¸ë˜í”„ ì‹¤í–‰ ê²°ê³¼(output) ì €ì¥           
            results.append({
                "original_clause": chunk,                               # ì¡°í•­ ì›ë³¸  
                "fairness_label": output.get('fairness_label', 'N/A'),   #  íŒë³„
                "unfair_type": output.get('unfair_type', 'â€”'),          # ë¶ˆê³µì • ìœ í˜•
                "improvement_proposal": output.get('improvement_proposal', 'â€”'),        # ê°œì„  ì œì•ˆ
                "related_cases_count": len(output.get('retrieved_cases_metadata', []))  # ì°¸ê³  ì‚¬ë¡€ ìˆ˜
            })

        except Exception as e:
            st.error(f"'ì¡°í•­ {i+1}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            results.append({
                "original_clause": chunk,
                "fairness_label": "ì˜¤ë¥˜",
                "unfair_type": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}",
                "improvement_proposal": "â€”",
                "related_cases_count": 0,
            })
            
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸
        progress_bar.progress((i + 1) / len(chunks), text=f"ë¶„ì„ ì§„í–‰ ì¤‘... ({i+1}/{len(chunks)})")

    progress_bar.empty()
    st.success("ëª¨ë“  ì¡°í•­ ë¶„ì„ ì™„ë£Œ!")
    
    # 4. ê²°ê³¼ ë¦¬í¬íŠ¸ í‘œì‹œ (ìƒˆë¡œ ì¶”ê°€í•œ í•¨ìˆ˜ í˜¸ì¶œ)
    display_batch_results(results)
      
def display_batch_results(results: List[dict]):
    """
    ì¼ê´„ ë¶„ì„ ê²°ê³¼ë¥¼ Streamlit UIì— ë¦¬í¬íŠ¸ í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    """
    # 1. (ìˆ˜ì •) 'unfair_type'ì´ ì•„ë‹Œ 'fairness_label'ì„ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
    problematic_clauses = [
        r for r in results 
        if r['fairness_label'] == "ë¶ˆê³µì •" # 'ë¶ˆê³µì •' ì¡°í•­ë§Œ í•„í„°ë§
    ]
    
    st.header(f"ê²€í†  ê²°ê³¼: ì´ {len(results)}ê°œ ì¡°í•­ ì¤‘ {len(problematic_clauses)}ê°œì˜ ë¶ˆê³µì • ì˜ì‹¬ ì¡°í•­ ë°œê²¬")
    st.divider()
    
    # ì¶”ê°€ 11/16
    if not problematic_clauses:
        st.success("íŠ¹ë³„íˆ ë¶ˆê³µì •ìœ¼ë¡œ ì˜ì‹¬ë˜ëŠ” ì¡°í•­ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    if problematic_clauses:
        st.subheader("ë¶ˆê³µì • ì˜ì‹¬ ì¡°í•­ ìƒì„¸")
        # 2. ì˜ì–´ í‚¤('unfair_type', 'original_clause', 'improvement_proposal') ì‚¬ìš©
        for i, res in enumerate(problematic_clauses):
            with st.expander(f"ì˜ì‹¬ ì¡°í•­ {i+1}: ({res['unfair_type']}) - {res['original_clause'][:50]}..."):
                
                # st.markdown()ì„ ì‚¬ìš©í•˜ì—¬ Markdown ì„œì‹ì„ ê·¸ëŒ€ë¡œ ë Œë”ë§
                st.markdown(res['improvement_proposal'], unsafe_allow_html=True)
                
    
# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def run_pdf_batch_mode(app, vectorstore, current_threshold_value):
    st.header("PDF ì•½ê´€ ì „ì²´ ê²€í† ")
    st.info("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¬¸ì„œ ì „ì²´ë¥¼ ë¶„ì„í•˜ì—¬ 'ë¶ˆê³µì • ì˜ì‹¬ ì¡°í•­' ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤.")

    uploaded_file = st.file_uploader(
        "ğŸ“„ ê²€í† í•  PDF ì•½ê´€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", 
        type="pdf",
        key="pdf_uploader" # keyë¥¼ ì¶”ê°€í•˜ì—¬ íƒ­ ì „í™˜ ì‹œ íŒŒì¼ì´ ìœ ì§€ë˜ë„ë¡ í•¨
    )
    
    if uploaded_file is not None:
        # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        pdf_text = utils.extract_text_from_pdf(uploaded_file)
        
        # 2. í…ìŠ¤íŠ¸ ë¶„í•  (Chunking)
        chunks = utils.split_text_into_clauses(pdf_text)
        
        st.markdown(f"ì´ {len(chunks)}ê°œì˜ ì¡°í•­(Chunk)ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        if st.button("ì „ì²´ ì¡°í•­ ë¶„ì„ ì‹œì‘í•˜ê¸°", type="primary", key="batch_start_btn"):
            # 3. vectorstoreë¥¼ run_batch_analysisë¡œ ì „ë‹¬
            run_batch_analysis(app, chunks, current_threshold_value, vectorstore)
